{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZgz/ruVfgox0T3+PK6xYW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Exercise 9 - Solution"],"metadata":{"id":"ZiyrvAbTE3kE"}},{"cell_type":"markdown","source":["### Task\n","\n","Implement a multilayer Fuly Connected Network from scratch using Numpy. Use the class structure provided below. Four member functions are to be implemented.\n","* 'forward(self, x)' for the forward prediction\n","* 'backward(self, y)' for the backpropagation in order to compute the gradients\n","* 'zero_grad(self)' to reset the gradients to zero (to be used before a second prediction/backpropagation)\n","* 'step(self, lr)' to update the weights and biases with gradient descent\n","\n","The most challenging task is to compute the gradients. Therefore split the implementation up into two steps:\n","\n","* implement the gradient computation for a sample size of one\n","* implement the gradient computation for an arbitrary sample size (use the currentWeightGradients and currentBiasGradients to store the intermediate results)\n","\n","To check the results, you can verify the gradients by comparing them to PyTorch. A code copying the custom neural network PyTorch, where the automatic differentiation is provided below.\n","\n","After a sccessful verification, use the neural network to learn a function. The training algorithm is provided below. Using the Adam optimizer implementation from exercise 2.3, the training can be improved. It is almost impossible to learn a sufficiently complex function with only gradient descent.\n","\n","Finally, compare the implementation to a PyTorch implementation."],"metadata":{"id":"FT8MiELpFGOy"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","import time"],"metadata":{"id":"LmDBPcRIO5IL","executionInfo":{"status":"ok","timestamp":1751310402529,"user_tz":180,"elapsed":5125,"user":{"displayName":"C치ssio Belo Clemente de Souza","userId":"18250501760700124894"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["neural network class"],"metadata":{"id":"05HxaC15O_3U"}},{"cell_type":"code","source":["class neuralNetwork:\n","    def __init__(\n","            self, layers, activation, activationGradient, xavierInitialization=True\n","    ):\n","        self.L = len(layers)\n","        if xavierInitialization == True:\n","            self.weights = [\n","                torch.nn.init.xavier_uniform_(\n","                    torch.zeros((layers[i], layers[i + 1])),\n","                    gain=torch.nn.init.calculate_gain(\"sigmoid\"),\n","                ).numpy()\n","                for i in range(self.L - 1)\n","            ]\n","        else:\n","            self.weights = [\n","                np.random.rand(layers[i], layers[i + 1]) for i in range(self.L - 1)\n","            ]\n","        self.biases = [np.random.rand(1, layers[i + 1]) for i in range(self.L - 1)]\n","\n","        self.layerActivations = []\n","\n","        self.weightGradients = [\n","            np.zeros((layers[i], layers[i + 1])) for i in range(self.L - 1)\n","        ]\n","        self.biasGradients = [np.zeros((1, layers[i + 1])) for i in range(self.L - 1)]\n","\n","        self.currentWeightGradients = [\n","            np.zeros((layers[i], layers[i + 1])) for i in range(self.L - 1)\n","        ]\n","        self.currentBiasGradients = [\n","            np.zeros((1, layers[i + 1])) for i in range(self.L - 1)\n","        ]\n","\n","        self.activation = activation\n","        self.activationGradient = activationGradient\n","\n","    def forward(self, x):\n","        self.layerActivations = []\n","        a = x\n","        self.layerActivations.append(a)\n","        for i in range(self.L - 1):\n","            z = a @ self.weights[i] + self.biases[i]\n","            self.layerActivations.append(z)\n","            a = self.activation(z)\n","        return a\n","\n","    def backward(self, y):\n","        if len(self.layerActivations) > 0:\n","            numberOfSamples = len(self.layerActivations[0])\n","\n","            if numberOfSamples == 1:\n","                deltaL = -(\n","                        y - self.activation(self.layerActivations[self.L - 1])\n","                ) * self.activationGradient(self.layerActivations[self.L - 1])\n","                self.biasGradients[self.L - 2] = deltaL\n","                for i in range(\n","                        self.L - 2\n","                ):\n","                    deltal = np.sum(\n","                        self.weights[self.L - 2 - i]\n","                        * self.biasGradients[self.L - 2 - i],\n","                        1,\n","                    ) * self.activationGradient(self.layerActivations[self.L - 2 - i])\n","                    self.biasGradients[self.L - 3 - i] = deltal\n","\n","                self.weightGradients[0] = (\n","                        np.transpose(self.layerActivations[0]) @ self.biasGradients[0]\n","                )\n","                for i in range(1, self.L - 1):\n","                    self.weightGradients[i] = (\n","                            np.transpose(self.activation(self.layerActivations[i]))\n","                            @ self.biasGradients[i]\n","                    )\n","\n","            elif numberOfSamples > 1:\n","                for j in range(numberOfSamples):\n","\n","                    deltaL = -(\n","                            y[j: j + 1]\n","                            - self.activation(self.layerActivations[self.L - 1][j: j + 1])\n","                    ) * self.activationGradient(\n","                        self.layerActivations[self.L - 1][j: j + 1]\n","                    )\n","                    self.currentBiasGradients[self.L - 2] = (\n","                            deltaL / numberOfSamples\n","                    )\n","                    self.biasGradients[self.L - 2] += self.currentBiasGradients[\n","                        self.L - 2\n","                        ]\n","                    for i in range(\n","                            self.L - 2\n","                    ):\n","                        deltal = np.sum(\n","                            self.weights[self.L - 2 - i]\n","                            * self.currentBiasGradients[self.L - 2 - i],\n","                            1,\n","                        ) * self.activationGradient(\n","                            self.layerActivations[self.L - 2 - i][j: j + 1]\n","                        )\n","                        self.currentBiasGradients[self.L - 3 - i] = deltal\n","                        self.biasGradients[self.L - 3 - i] += self.currentBiasGradients[\n","                            self.L - 3 - i\n","                            ]\n","\n","                    self.currentWeightGradients[0] = (\n","                            np.transpose(self.layerActivations[0][j: j + 1])\n","                            @ self.currentBiasGradients[0]\n","                    )\n","                    self.weightGradients[0] += self.currentWeightGradients[0]\n","                    for i in range(1, self.L - 1):\n","                        self.currentWeightGradients[i] = (\n","                                np.transpose(\n","                                    self.activation(self.layerActivations[i][j: j + 1])\n","                                )\n","                                @ self.currentBiasGradients[i]\n","                        )\n","                        self.weightGradients[i] += self.currentWeightGradients[i]\n","\n","        else:\n","            print(\"backward propagation not possible\")\n","\n","    def zero_grad(self):\n","        self.weightGradients = [\n","            np.zeros((layers[i], layers[i + 1])) for i in range(self.L - 1)\n","        ]\n","        self.biasGradients = [np.zeros((1, layers[i + 1])) for i in range(self.L - 1)]\n","\n","    def step(self, lr):\n","        for i in range(self.L - 1):\n","            self.weights[i] -= lr * self.weightGradients[i]\n","            self.biases[i] -= lr * self.biasGradients[i]"],"metadata":{"id":"rUSGu_pDFA8J","executionInfo":{"status":"ok","timestamp":1751311435364,"user_tz":180,"elapsed":54,"user":{"displayName":"C치ssio Belo Clemente de Souza","userId":"18250501760700124894"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["model definition"],"metadata":{"id":"TpoybX35Tp_s"}},{"cell_type":"code","source":["layers = [1, 4, 4, 1]\n","sigmoid = lambda x: 1 / (1 + np.exp(-x))\n","sigmoidGradient = lambda x: sigmoid(x) * (1 - sigmoid(x))\n","\n","model = neuralNetwork(layers, sigmoid, sigmoidGradient)\n","\n","x = np.expand_dims(np.linspace(0, 1, 2), 1) + 0.2\n","y = np.sin(4 * np.pi * x) ** 2"],"metadata":{"id":"wJ4zWbN1S8SW","executionInfo":{"status":"ok","timestamp":1751311883260,"user_tz":180,"elapsed":60,"user":{"displayName":"C치ssio Belo Clemente de Souza","userId":"18250501760700124894"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["prediction, cost computation & gradient computation"],"metadata":{"id":"jfMA4hnnUJi0"}},{"cell_type":"code","source":["yPred = model.forward(x)\n","\n","C = 0.5 * np.mean((yPred - y) ** 2)\n","\n","model.backward(y)"],"metadata":{"id":"ln0JRRFvUJ9B","executionInfo":{"status":"ok","timestamp":1751311889872,"user_tz":180,"elapsed":3,"user":{"displayName":"C치ssio Belo Clemente de Souza","userId":"18250501760700124894"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Verification with PyTorch"],"metadata":{"id":"pJXhtfW7UO-d"}},{"cell_type":"markdown","source":["model definition and cloning of model parameters"],"metadata":{"id":"3HFWyaA9UTV8"}},{"cell_type":"code","source":["class neuralNetworkTorch(torch.nn.Module):\n","    def __init__(self, layers, activationFunction=torch.nn.Sigmoid()):\n","        super().__init__()\n","        modules = []\n","        for i in range(len(layers) - 1):\n","            modules.append(torch.nn.Linear(layers[i], layers[i + 1]))\n","            modules.append(activationFunction)\n","\n","        self.model = torch.nn.Sequential(*modules)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","\n","modelTorch = neuralNetworkTorch(layers)\n","\n","with torch.no_grad():\n","    for i, param in enumerate(modelTorch.parameters()):\n","        if i % 2 == 0:\n","            param.data = torch.from_numpy(model.weights[i // 2]).to(torch.float64).t()\n","        else:\n","            param.data = torch.from_numpy(model.biases[i // 2]).to(torch.float64)"],"metadata":{"id":"65mWvZ1zURiE","executionInfo":{"status":"ok","timestamp":1751311896007,"user_tz":180,"elapsed":29,"user":{"displayName":"C치ssio Belo Clemente de Souza","userId":"18250501760700124894"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["prediction, cost computation & gradient computation"],"metadata":{"id":"l5XQYiiqUZu0"}},{"cell_type":"code","source":["xTorch = torch.from_numpy(x).to(torch.float64)\n","yPredTorch = modelTorch.forward(xTorch)\n","\n","CTorch = 0.5 * torch.mean((yPredTorch - torch.from_numpy(y).to(torch.float64)) ** 2)\n","CTorch.backward()"],"metadata":{"id":"TrE2Qr03URBO","executionInfo":{"status":"ok","timestamp":1751311898634,"user_tz":180,"elapsed":62,"user":{"displayName":"C치ssio Belo Clemente de Souza","userId":"18250501760700124894"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["gradient comparison"],"metadata":{"id":"q4efeiH4U2Il"}},{"cell_type":"code","source":["layer = 0\n","print(\"weight:\")\n","print(np.transpose(model.weightGradients[layer]))\n","print(list(modelTorch.parameters())[2 * layer].grad)\n","\n","print(\"bias:\")\n","print(model.biasGradients[layer])\n","print(list(modelTorch.parameters())[2 * layer + 1].grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ojNh9Qj-UtZh","executionInfo":{"status":"ok","timestamp":1751311950972,"user_tz":180,"elapsed":28,"user":{"displayName":"C치ssio Belo Clemente de Souza","userId":"18250501760700124894"}},"outputId":"8f9a361b-4aca-4791-dd36-905027b364f0"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["weight:\n","[[-8.20482858e-06]\n"," [-1.11781064e-04]\n"," [ 2.74173667e-04]\n"," [ 2.01863001e-04]]\n","tensor([[-8.2048e-06],\n","        [-1.1178e-04],\n","        [ 2.7417e-04],\n","        [ 2.0186e-04]], dtype=torch.float64)\n","bias:\n","[[-1.12428905e-05 -1.95147987e-04  4.34993078e-04  2.75672713e-04]]\n","tensor([[-1.1243e-05, -1.9515e-04,  4.3499e-04,  2.7567e-04]],\n","       dtype=torch.float64)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"r6be5-UyU6Ls"},"execution_count":null,"outputs":[]}]}