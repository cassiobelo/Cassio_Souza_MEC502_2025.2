{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN1edfnbWtVa1ZWtGwIUP9Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Implementar exemplos de Redes Neurais usando PyTorch, variando hiperparâmetros de treinamento."],"metadata":{"id":"VueMBRWXhKvI"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wjID-BrH-PYE18nkFjcRz9gEIUsxlEg5"},"id":"FntH6rPyhDGf","executionInfo":{"status":"ok","timestamp":1751583911348,"user_tz":180,"elapsed":288220,"user":{"displayName":"Cássio Belo Clemente de Souza","userId":"18250501760700124894"}},"outputId":"35f8b866-8c53-46f9-c491-cb12f2e92c87"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.datasets import make_moons # Para gerar dados não lineares\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","print(\"--- Iniciando Exemplos de Redes Neurais com Variação de Hiperparâmetros ---\")\n","\n","# 1. Gerar Dados Sintéticos para Classificação Binária (Exemplo: Luas)\n","# Usaremos make_moons para criar um conjunto de dados não linear, ideal para redes neurais\n","X, y = make_moons(n_samples=200, noise=0.15, random_state=42)\n","X = torch.tensor(X, dtype=torch.float32)\n","y = torch.tensor(y, dtype=torch.float32).view(-1, 1) # Redimensionar para (N, 1)\n","\n","# Dividir dados em treinamento e teste\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Visualizar os dados gerados\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_train[:, 0].numpy(), X_train[:, 1].numpy(), c=y_train.numpy().flatten(), cmap='viridis', label='Dados de Treinamento')\n","plt.scatter(X_test[:, 0].numpy(), X_test[:, 1].numpy(), c=y_test.numpy().flatten(), cmap='plasma', marker='x', s=100, label='Dados de Teste')\n","plt.title('Dados Sintéticos de Classificação (Luas)')\n","plt.xlabel('Característica 1')\n","plt.ylabel('Característica 2')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# 2. Definir o Modelo da Rede Neural\n","class SimpleNN(nn.Module):\n","    def __init__(self, activation_fn=nn.ReLU):\n","        super(SimpleNN, self).__init__()\n","        # Camada de entrada (2 características) para camada oculta (64 neurônios)\n","        self.fc1 = nn.Linear(2, 64)\n","        # Função de ativação (pode ser ReLU, Sigmoid, Tanh, etc.)\n","        self.activation = activation_fn()\n","        # Camada oculta (64 neurônios) para camada de saída (1 neurônio para classificação binária)\n","        self.fc2 = nn.Linear(64, 1)\n","        # Sigmoid para a camada de saída para produzir probabilidades entre 0 e 1\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        out = self.fc1(x)\n","        out = self.activation(out)\n","        out = self.fc2(out)\n","        out = self.sigmoid(out)\n","        return out\n","\n","# 3. Função de Treinamento\n","def train_model(model, X_train, y_train, X_test, y_test, criterion, optimizer_class, lr, num_epochs, batch_size):\n","    optimizer = optimizer_class(model.parameters(), lr=lr)\n","    train_losses = []\n","    test_losses = []\n","\n","    # Criar DataLoader para batching\n","    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","    for epoch in range(num_epochs):\n","        model.train() # Define o modelo para modo de treinamento\n","        current_train_loss = 0.0\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            current_train_loss += loss.item() * inputs.size(0) # Acumula perda ponderada pelo tamanho do batch\n","\n","        train_loss_avg = current_train_loss / len(train_dataset)\n","        train_losses.append(train_loss_avg)\n","\n","        # Avaliar no conjunto de teste\n","        model.eval() # Define o modelo para modo de avaliação\n","        with torch.no_grad(): # Desativa o cálculo de gradientes\n","            test_outputs = model(X_test)\n","            test_loss = criterion(test_outputs, y_test).item()\n","            test_losses.append(test_loss)\n","\n","        if (epoch + 1) % (num_epochs // 10) == 0:\n","            print(f'  Época [{epoch+1}/{num_epochs}], Perda de Treinamento: {train_loss_avg:.4f}, Perda de Teste: {test_loss:.4f}')\n","    return train_losses, test_losses\n","\n","# 4. Variação de Hiperparâmetros e Visualização\n","\n","# Hiperparâmetros base\n","base_lr = 0.01\n","base_epochs = 1000\n","base_batch_size = 32\n","base_optimizer = optim.Adam\n","base_activation = nn.ReLU\n","base_criterion = nn.BCELoss() # Binary Cross-Entropy Loss para classificação binária\n","\n","# Função para plotar os resultados\n","def plot_results(title, train_losses, test_losses, model, X_test, y_test):\n","    plt.figure(figsize=(15, 6))\n","\n","    # Plotar as perdas\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_losses, label='Perda de Treinamento')\n","    plt.plot(test_losses, label='Perda de Teste')\n","    plt.title(f'Perda ao longo das Épocas - {title}')\n","    plt.xlabel('Época')\n","    plt.ylabel('Perda')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Plotar a fronteira de decisão\n","    plt.subplot(1, 2, 2)\n","    # Criar um grid de pontos para prever\n","    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n","    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n","    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n","                         np.linspace(y_min, y_max, 100))\n","    grid_points = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        Z = model(grid_points).numpy().reshape(xx.shape)\n","        Z = (Z > 0.5).astype(int) # Converte probabilidades para classes (0 ou 1)\n","\n","    plt.contourf(xx, yy, Z, alpha=0.8, cmap='coolwarm')\n","    plt.scatter(X_train[:, 0].numpy(), X_train[:, 1].numpy(), c=y_train.numpy().flatten(), cmap='viridis', edgecolors='k', s=20, label='Treinamento')\n","    plt.scatter(X_test[:, 0].numpy(), X_test[:, 1].numpy(), c=y_test.numpy().flatten(), cmap='plasma', marker='x', s=100, label='Teste')\n","    plt.title(f'Fronteira de Decisão - {title}')\n","    plt.xlabel('Característica 1')\n","    plt.ylabel('Característica 2')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","\n","# --- Variação da Taxa de Aprendizado (Learning Rate) ---\n","print(\"\\n--- Testando diferentes Taxas de Aprendizado (Learning Rate) ---\")\n","learning_rates = [0.1, 0.01, 0.001]\n","for lr in learning_rates:\n","    print(f\"  Treinando com LR = {lr}\")\n","    model = SimpleNN(activation_fn=base_activation)\n","    train_losses, test_losses = train_model(model, X_train, y_train, X_test, y_test,\n","                                            base_criterion, base_optimizer, lr, base_epochs, base_batch_size)\n","    plot_results(f'LR = {lr}', train_losses, test_losses, model, X_test, y_test)\n","\n","# --- Variação do Número de Épocas ---\n","print(\"\\n--- Testando diferentes Números de Épocas ---\")\n","epochs_list = [200, 1000, 3000]\n","for epochs in epochs_list:\n","    print(f\"  Treinando com {epochs} Épocas\")\n","    model = SimpleNN(activation_fn=base_activation)\n","    train_losses, test_losses = train_model(model, X_train, y_train, X_test, y_test,\n","                                            base_criterion, base_optimizer, base_lr, epochs, base_batch_size)\n","    plot_results(f'Épocas = {epochs}', train_losses, test_losses, model, X_test, y_test)\n","\n","# --- Variação do Tamanho do Batch (Batch Size) ---\n","print(\"\\n--- Testando diferentes Tamanhos de Batch ---\")\n","batch_sizes = [1, 32, 64, len(X_train)] # 1 (SGD puro), 32 (comum), 64, tamanho total (Batch Gradient Descent)\n","for bs in batch_sizes:\n","    print(f\"  Treinando com Batch Size = {bs}\")\n","    model = SimpleNN(activation_fn=base_activation)\n","    train_losses, test_losses = train_model(model, X_train, y_train, X_test, y_test,\n","                                            base_criterion, base_optimizer, base_lr, base_epochs, bs)\n","    plot_results(f'Batch Size = {bs}', train_losses, test_losses, model, X_test, y_test)\n","\n","# --- Variação do Otimizador ---\n","print(\"\\n--- Testando diferentes Otimizadores ---\")\n","optimizers = {'SGD': optim.SGD, 'Adam': optim.Adam}\n","for name, opt_class in optimizers.items():\n","    print(f\"  Treinando com Otimizador = {name}\")\n","    model = SimpleNN(activation_fn=base_activation)\n","    train_losses, test_losses = train_model(model, X_train, y_train, X_test, y_test,\n","                                            base_criterion, opt_class, base_lr, base_epochs, base_batch_size)\n","    plot_results(f'Otimizador = {name}', train_losses, test_losses, model, X_test, y_test)\n","\n","# --- Variação da Função de Ativação ---\n","print(\"\\n--- Testando diferentes Funções de Ativação ---\")\n","activations = {'ReLU': nn.ReLU, 'Sigmoid': nn.Sigmoid, 'Tanh': nn.Tanh}\n","for name, act_fn in activations.items():\n","    print(f\"  Treinando com Função de Ativação = {name}\")\n","    model = SimpleNN(activation_fn=act_fn)\n","    train_losses, test_losses = train_model(model, X_train, y_train, X_test, y_test,\n","                                            base_criterion, base_optimizer, base_lr, base_epochs, base_batch_size)\n","    plot_results(f'Função de Ativação = {name}', train_losses, test_losses, model, X_test, y_test)\n","\n","print(\"\\n--- Exemplos de Variação de Hiperparâmetros Concluídos ---\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Jk-SgUk8hOmZ"},"execution_count":null,"outputs":[]}]}